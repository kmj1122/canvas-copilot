WEBVTT

1
00:00:00.250 --> 00:00:03.239
Joe Grand (Kingpin): But yeah, so he's gonna go by himself, which is pretty cool.

2
00:00:03.390 --> 00:00:05.459
Ryan Wright: Is ben 16. Now, is that right?

3
00:00:05.460 --> 00:00:14.319
Joe Grand (Kingpin): 16. Yeah, so he can go. He can fly alone. Without the requirement of like a unaccompanied minor.

4
00:00:14.610 --> 00:00:17.430
Joe Grand (Kingpin): And he did it a couple weeks ago to go to San Francisco.

5
00:00:17.430 --> 00:00:24.750
Ryan Wright: That's awesome. Yeah. 15. We still have to do the drop them off at the gate, do all that with her.

6
00:00:24.750 --> 00:00:28.285
Ryan Wright: you know, but soon soon to be 16.

7
00:00:28.920 --> 00:00:31.919
Joe Grand (Kingpin): It's pretty crazy. Yeah, it's pretty wild to see.

8
00:00:32.320 --> 00:00:36.399
Ryan Wright: Awesome. So are you ready to get started? I want to.

9
00:00:36.400 --> 00:00:39.540
Joe Grand (Kingpin): For sure, is it? Does this look and sound? Okay.

10
00:00:39.540 --> 00:00:42.249
Ryan Wright: Yeah, you look and sound great as as normal. Joe.

11
00:00:42.853 --> 00:00:43.520
Joe Grand (Kingpin): Cool thanks.

12
00:00:43.520 --> 00:00:49.090
Ryan Wright: You have a you have a different hairstyle every time we do this, which I which I appreciate now you're toking it up, which.

13
00:00:49.090 --> 00:01:01.699
Joe Grand (Kingpin): Well, it's freezing out here. It's like 40 degrees and raining. And my office is like, I have heat. But I don't wanna make it too hot. Yeah. And actually, we just lost our heat in our house because our hot water heater broke, and

14
00:01:02.180 --> 00:01:08.239
Joe Grand (Kingpin): and that and our heat is through radiant flooring, heating. So yeah, it's it's gonna be a cold, wet

15
00:01:08.620 --> 00:01:10.620
Joe Grand (Kingpin): Thanksgiving week, that's for sure.

16
00:01:10.620 --> 00:01:11.160
Ryan Wright: Yeah.

17
00:01:11.270 --> 00:01:32.680
Ryan Wright: that's awesome. So let me do a quick introduction. There's a couple of questions I have as as students pop in. We're we're right at time for this. So we're not used to zoom. So I'm just gonna remind everybody kind of the zoom etiquette for us is cameras on hands up that kind of stuff just so I can control things because I'm not used to zoom either

18
00:01:32.680 --> 00:02:00.879
Ryan Wright: as well. So I am thrilled that Joe Grand has come back and and found some time for us. Thanks, Joe, for doing this quick introduction. So, as you know, they viewed a couple of your videos, you know, when you're talking about loft, heavy industries, you're testifying before the Us. Senate which they're interested in your engineering background, that we were just going through for those who are popping in the kind of cool experiment he's running

19
00:02:00.900 --> 00:02:23.139
Ryan Wright: right now. He was on the Discovery channel on a show called Prototype. This you're gonna have to tell me more about that. That's very cool. One of the cool things about Joe is Joe. This is gonna date. How old you and I are. But about 20 years ago he was, Joe introduced me to the first, st ever 3D printer that I ever saw

20
00:02:23.140 --> 00:02:32.909
Ryan Wright: at his lab as well. Which was very cool in San Francisco, but he is not even, I think, the most talented person in his family.

21
00:02:32.910 --> 00:02:55.209
Ryan Wright: his wife, which I'll drop the chat in is a pretty talented author as well, and has recently published this. I think, super cool book troublemakers and superpowers. So I know how it's like being number 2 in my family as well, Joe. As well. But it's it's very cool, very cool family. We've known each other for a long time, and I think.

22
00:02:55.230 --> 00:03:08.579
Joe Grand (Kingpin): I wouldn't be remiss doing a cyber security class without talking to Joe Grand. So let's welcome Joe as well for a couple of minutes on zoom with us as we're we're all all about the world. So welcome, Joe.

23
00:03:08.580 --> 00:03:20.460
Joe Grand (Kingpin): Thank you. Thanks again for having me. I remember we did this last year, and it was like everybody was still in the classroom. So I guess different year. Different class and Thanksgiving coming up. But yeah, thanks for having me.

24
00:03:20.680 --> 00:03:49.490
Ryan Wright: So I would love. So we sent. We've been talking a whole bunch of different questions with the class and how we start with. So there's a couple ones that many of us want to know, and then I'll just like before we'll do just a Q. And a. I'd love for you to just redo that experiment that you're running for the class that you showed me at the beginning as people are popping in. I think that's super cool, but I want to start right at the beginning, and there's a couple of questions like.

25
00:03:49.490 --> 00:03:51.390
Ryan Wright: Tell me about your 1st hack.

26
00:03:51.690 --> 00:04:03.150
Joe Grand (Kingpin): Okay? Yeah. So I've I've been involved in electronics and computers since 1982, I should probably stop saying that. And then just say what age I was, because

27
00:04:03.150 --> 00:04:03.790
Joe Grand (Kingpin): I don't know

28
00:04:03.790 --> 00:04:28.499
Joe Grand (Kingpin): won't sound as old. But yeah, when I was 7 years old, and so I just always loved kind of I've just been able to relate to like computers and electronics. But at the time, so I was exploring a lot with like bulletin board systems and connecting to other computer systems with my modem and kind of learning about hacking, you know, starting collecting games and then kind of moving on to building projects. So my 1st hack

29
00:04:28.780 --> 00:04:47.070
Joe Grand (Kingpin): was sort of related. So there wasn't like the Internet. It was computer systems, you know, you could connect one on one. So the telephone system was really like the thing that you would mess with. That was like the largest network at the time. And so I was just really into telephone stuff in my 1st hack

30
00:04:47.140 --> 00:04:55.092
Joe Grand (Kingpin): was a little device, and I didn't design it, but I read about it in like a hacker text file

31
00:04:56.430 --> 00:05:06.609
Joe Grand (Kingpin): the hacker text file. You know, people would write files instead of like blog posts. It would be like a a file, and they'd share it around with people. This is one that you could plug into your phone line.

32
00:05:06.800 --> 00:05:23.819
Joe Grand (Kingpin): So the phone you could still use the phone as normal. So like, my parents could call people. And I could use the phone. But nobody could call our house. So it would basically be a busy signal if somebody tried to call us so nobody could. You know the phone wouldn't ring in my house, but then we could use it. And

33
00:05:24.090 --> 00:05:40.295
Joe Grand (Kingpin): I thought that would be cool. So like teachers from school wouldn't be able to call, and I was doing a lot of like a lot of stuff with the telephone as far as figuring out how to make free phone calls and connecting to hacked voicemail systems where people would trade information on someone else's voicemail box. And

34
00:05:40.960 --> 00:05:51.610
Joe Grand (Kingpin): sometimes you would get a call back from these organizations and from like the telephone operator. So having this device would basically prevent me from getting in trouble because nobody could call my house. It was perfect.

35
00:05:51.900 --> 00:05:52.770
Ryan Wright: That's awesome.

36
00:05:52.770 --> 00:06:03.009
Joe Grand (Kingpin): And that was also my 1st circuit boards. It was like this tiny little thing, and that sort of just, you know, spurned my whole interest in getting involved in, in, like? What other mischievous stuff could I build? And could I do.

37
00:06:04.080 --> 00:06:32.699
Ryan Wright: This might surprise you, Joe, but at a business school the number one thing that people were concerned about was the ethics of what you do and like how you've kind of morphed in between all these different ways that the gray, the black, and others. I'm interested in the role that you think ethics plays in hacking in general? That seems to be a question that everybody's interested in.

38
00:06:32.700 --> 00:06:40.770
Ryan Wright: and any comments you have on, how companies are are using or being ethically appropriate for things like this.

39
00:06:41.140 --> 00:06:44.075
Joe Grand (Kingpin): I mean, I think ethics is a huge part of it. And

40
00:06:44.480 --> 00:06:49.500
Joe Grand (Kingpin): I think that's changed over time, like when I 1st got involved

41
00:06:49.780 --> 00:06:57.249
Joe Grand (Kingpin): in hacking, and I was part of a hacking group called the Loft, and that was one of the you know that we did the Senate testimony. The 1st publicly kind of facing Hacker group.

42
00:06:57.270 --> 00:07:03.390
Joe Grand (Kingpin): It was very important for us to be able to not just rely on the company to

43
00:07:03.520 --> 00:07:05.800
Joe Grand (Kingpin): fix a problem if we found one, but

44
00:07:06.030 --> 00:07:11.389
Joe Grand (Kingpin): that we thought we were being ethically responsible by sharing those problems with the world.

45
00:07:11.440 --> 00:07:20.233
Joe Grand (Kingpin): so users could protect themselves instead of waiting for the companies to do it. And there wasn't really an organized crime, you know,

46
00:07:20.960 --> 00:07:26.804
Joe Grand (Kingpin): side of things and ransomware and all these groups making money. So it was just different.

47
00:07:27.970 --> 00:07:30.689
Joe Grand (Kingpin): but now it's a little bit of a

48
00:07:30.720 --> 00:07:35.359
Joe Grand (Kingpin): it's a it's a different sort of dilemma, because if you release something publicly

49
00:07:35.400 --> 00:07:38.579
Joe Grand (Kingpin): that does educate users, but it also educates.

50
00:07:39.232 --> 00:07:53.340
Joe Grand (Kingpin): You know, the bad guys to actually use that and exploit it before the company can fix it. But at the same time a lot of companies don't want still don't want to hear that their baby is ugly.

51
00:07:53.440 --> 00:07:59.400
Joe Grand (Kingpin): and if you find a bug in their in their software, a security problem in their in their software or in their hardware.

52
00:07:59.880 --> 00:08:22.380
Joe Grand (Kingpin): a lot of times they're like they either ignore it, which is what they used to do with the loft, or they'll, you know. Have you sign some nondisclosure agreement, and they'll make you wait a long amount of time, so they could do whatever they want to do to try to fix the problem. Sometimes they don't, so it's not as clear cut as it as it used to be but I I've always had this very strong, like hacker ethos of.

53
00:08:22.450 --> 00:08:27.019
Joe Grand (Kingpin): even though I I was getting in trouble when I was younger. But it really is sort of like.

54
00:08:27.180 --> 00:08:44.530
Joe Grand (Kingpin): you know, kind of do no harm, exploring and and finding problems in devices and and technology, and then helping people and sharing that information so other people can learn from it and do something with. But it definitely is something. Now, as a researcher, you have to be a lot more careful about what

55
00:08:44.580 --> 00:09:08.975
Joe Grand (Kingpin): you release, because there's sort of these unanticipated side effects that happen, you know, like, even when Microsoft releases patches, you know weekly patches, or whatever people look through those and then say, Oh, how can I exploit whatever they just patched? Did they? Did they patch a security problem. So the world is just a much different place where I would like to think that all of us would do the right thing with information that gets released. But

56
00:09:09.390 --> 00:09:13.269
Joe Grand (Kingpin): that's you know, that's my idealistic view. And it it just doesn't happen anymore.

57
00:09:13.730 --> 00:09:19.910
Ryan Wright: Who who's doing it right. And and maybe if you, if you don't want to name names, then tell us how they're doing it right.

58
00:09:20.500 --> 00:09:39.089
Joe Grand (Kingpin): So I I think, cause I don't. I don't have a lot of play in the in the software world like with hardware. It's it's worse because a lot of times, if you have a hardware problem, it can't be patched. If it's something like, I'll explain what I'm working on next afterwards. But if you have a hardware problem, the company basically.

59
00:09:39.180 --> 00:09:43.819
Joe Grand (Kingpin): if it's at a chip level, they need to get new, they need to create new chips. If it's

60
00:09:43.860 --> 00:10:05.379
Joe Grand (Kingpin): maybe a firmware thing software patch, maybe they can patch it. But updating, you know, hardware devices is not as easy as pushing a patch to everybody that's on the Internet for a piece of software. So hardware is really like a scary world, because people don't really kind of know, the companies are not that willing to listen? Most of them, I would say, like

61
00:10:07.860 --> 00:10:18.580
Joe Grand (Kingpin): Microsoft, I think as much as everybody likes to Bash Microsoft. I think they really are sort of. They've been the ones that people have been going after the most, and they're also the most receptive with

62
00:10:18.690 --> 00:10:34.225
Joe Grand (Kingpin): responding to security problems and handling patches and doing all of these things. That was one of the 1st companies that we went head to head with back at the loft, and they were like, no, no one's ever gonna hack this. That's not a problem. And that was 20 something years ago, almost 30 years ago, I guess. And

63
00:10:35.470 --> 00:10:38.090
Joe Grand (Kingpin): they're actually like doing it right.

64
00:10:38.100 --> 00:10:52.379
Joe Grand (Kingpin): There's also, you know, you have bug bounty programs like hacker one. So companies that don't want to set up their own complete security response team, or whatever it is like. They can kind of outsource that to a place that would then

65
00:10:52.670 --> 00:10:59.025
Joe Grand (Kingpin): handle. You know any sort of incoming vulnerabilities and stuff. So there's opportunities for companies to do it right.

66
00:10:59.830 --> 00:11:02.939
Joe Grand (Kingpin): I just I just feel like it still needs to happen

67
00:11:03.260 --> 00:11:15.840
Joe Grand (Kingpin): more and more instead of companies like threatening to sue you as a researcher, which is just gonna it's just gonna quiet the good research. And it's gonna amplify the negative research which people just aren't going to publicize.

68
00:11:16.220 --> 00:11:20.829
Ryan Wright: Yeah, that's helpful. So I'm going to open it up to the students. I have one more question

69
00:11:20.930 --> 00:11:28.969
Ryan Wright: that, like 10 people ask, because it's what everybody's talking about. You probably even assume it like, how does AI change your life?

70
00:11:29.210 --> 00:11:29.870
Joe Grand (Kingpin): Yeah.

71
00:11:30.350 --> 00:11:44.029
Joe Grand (Kingpin): so it's funny because I avoided it for so long. Just because I was like this, whatever this is not something I need. And when I was working on, I don't know if you guys saw it was the the, my recent video on

72
00:11:44.240 --> 00:12:09.795
Joe Grand (Kingpin): hacking the roboform password generator, where I basically found a vulnerability, and how passwords were being generated for this old version of of this password generator. And then we were able to use that to recover this guy's cryptocurrency. Which actually is a great example, because that company didn't have any sort of security policy, you know. Like most most of the time you go to a website and and they say, like, if you have a security problem, contact security at whatever

73
00:12:10.890 --> 00:12:18.000
Joe Grand (Kingpin): or even any sort of submission form like this company who created the Robo Form package

74
00:12:18.560 --> 00:12:37.129
Joe Grand (Kingpin): had no information about how to submit a security vulnerability. I looked online like there was no prior any information about any prior public work they had done with researchers. And in this day and age like, that's a scary thing, right? Because, like, I just mentioned. Some companies don't want to admit that they have problems.

75
00:12:38.010 --> 00:12:43.269
Joe Grand (Kingpin): and that one, we decided to release this video and release the information.

76
00:12:43.590 --> 00:13:00.719
Joe Grand (Kingpin): And we gave Roboform a chance to respond. And we we contacted them and said, Hey, we found this problem no response. One journalist, actually, they did respond and said, Oh, that problem's been fixed. But they didn't answer how the problem was fixed and anything. So they that was sort of an example of kind of a negative.

77
00:13:01.850 --> 00:13:12.070
Joe Grand (Kingpin): you know, kind of response to that. But in that project, when I was doing that project and sort of reverse engineering, a lot of the code that was generating the the passwords.

78
00:13:12.410 --> 00:13:32.689
Joe Grand (Kingpin): I had heard that there was like this Plugin for this reverse engineering tool called Gedra, that I use that is created by the Nsa. Funny enough. And you can actually go through and reverse engineer binaries and look through it. And you know, figure out how the code's running. There was a plugin for that that would send a piece of code to. I think it was to chat Gpt

79
00:13:33.156 --> 00:13:35.819
Joe Grand (Kingpin): and then it would come back with like a text

80
00:13:35.960 --> 00:13:38.819
Joe Grand (Kingpin): description of what that piece of code did

81
00:13:38.900 --> 00:13:44.712
Joe Grand (Kingpin): which was actually really helpful. Because some of this stuff was really complicated. And

82
00:13:45.350 --> 00:13:57.521
Joe Grand (Kingpin): and it kind of hard to figure out. Given that I mostly do hardware, not software. So that was my 1st experience with it last year. And I was like, this is actually kind of cool, because it's taking some complex thing and kind of digesting it.

83
00:13:57.790 --> 00:14:02.690
Joe Grand (Kingpin): And then I've been using it more recently, for, like helping me write code for

84
00:14:02.850 --> 00:14:26.590
Joe Grand (Kingpin): the you know, different projects I'm working on where it's like, I give it a set of like, I need to do this, or I need to have these bounds of this, you know, for this in input data, I have to process it in this way. What do I do? And it comes back with something that kind of works, maybe sometimes, but it's a it's a starting point, and then I can go through and kind of refine it. So it's actually been helpful from that sort of coding assistance as a tool.

85
00:14:26.880 --> 00:14:31.330
Joe Grand (Kingpin): that I've actually been really surprised with, because it it just has saved

86
00:14:31.590 --> 00:14:41.999
Joe Grand (Kingpin): months of work. And you know a lot of searching through stack overflow, and all these other websites of trying to find problems and reading through Apis and documentation. So

87
00:14:42.080 --> 00:14:58.359
Joe Grand (Kingpin): that's that's what I use it for. I know there's a lot of other work with AI going into sort of like vulnerability, analysis, threat, threat, analysis, kind of things. And it's definitely interesting. But I wouldn't say I'm a complete convert. But I'm definitely like, I'm impressed.

88
00:14:58.670 --> 00:15:00.680
Ryan Wright: Yeah. You've taken the gateway.

89
00:15:01.170 --> 00:15:20.559
Joe Grand (Kingpin): Yeah, I mean it. And it actually serve. It served me properly, like I think it's scary as a whole, like, if we rely on it too much, you know, like anything else. It's it's technology has. You have this balance. And as humans, I feel like we tend to over over use it. But it's been yeah. So far, it's been like, really helpful.

90
00:15:20.940 --> 00:15:34.509
Ryan Wright: So you just showed me at the very beginning of this call something very cool and interesting. Would you mind sharing that with the class. I'd like to know why you're doing it, too, which you never got to. But yeah.

91
00:15:34.510 --> 00:15:53.289
Joe Grand (Kingpin): So this is so this is sort of a follow up. So you did. You showed them the wallet hacking video. Is that one of them. Yeah, okay, so this is a follow up to that. So when I did that video that I was just getting into fault injection and learning about this new technique that I'd never really worked with before, and got it to the point. That was good enough for Dan to come over and and hack his device.

92
00:15:53.900 --> 00:15:56.539
Joe Grand (Kingpin): After that video came out and it went viral.

93
00:15:56.600 --> 00:16:20.159
Joe Grand (Kingpin): We started getting emails from other people that needed help. And I started hacking a couple other treasures, and I was like doing some more research on it, because I wasn't fully comfortable with the attack, and it turns out that I was just super lucky with. With that day of of doing his like. There was a lot of risk of causing corruption and causing the the device to get erased. And all these things that you don't want to do on a customer's device. So for the past

94
00:16:20.300 --> 00:16:33.469
Joe Grand (Kingpin): 2 and a half years, 3 years, whatever it is, I've been refining the process, and I've I've moved to a different type of fault injection and created this whole platform. So this is what I'm working on now. It's not public yet, but it will be soon.

95
00:16:33.600 --> 00:16:45.740
Joe Grand (Kingpin): And I can actually do this because I'm on my second. I'm on a separate laptop because the laptop that's running. This thing is normally the one I use. But I'm on this because I don't want to mess it up. So this thing here is.

96
00:16:45.810 --> 00:16:55.940
Joe Grand (Kingpin): It's an electromagnetic fault injection platform. So I basically have, like the controller circuitry over here, bunch of stuff connected to my computer. And

97
00:16:55.960 --> 00:16:57.599
Joe Grand (Kingpin): let's see if I can zoom in on it.

98
00:16:58.210 --> 00:17:08.050
Joe Grand (Kingpin): So this tool here is basically injecting an electromagnetic pulse that I control into the chip that I'm trying to break the security of the chip.

99
00:17:08.140 --> 00:17:11.819
Joe Grand (Kingpin): and if I get the correct

100
00:17:12.190 --> 00:17:24.339
Joe Grand (Kingpin): characteristics as far as the the right amount of time and the the characteristics of the pulse that I'm putting in there in the right position of the chip or the right position of the probe over the chip, I could actually

101
00:17:24.800 --> 00:17:25.819
Joe Grand (Kingpin): cause

102
00:17:26.089 --> 00:17:43.790
Joe Grand (Kingpin): a corruption at just the right point in time where I can break the security of the chip to get access to the memory which would then give me access nowadays to the encrypted memory contents. But then I would go and crack offline to get the PIN and get the recovery seed. So it's kind of an extension of earlier work, but a much

103
00:17:43.800 --> 00:17:56.880
Joe Grand (Kingpin): more controllable method. And like I've learned so much. So right now I'm scanning a chip. So I'm doing some testing. And this is for the Trezor model. T, so it's a later version that uses a slightly different type of microcontroller.

104
00:17:57.120 --> 00:18:14.110
Joe Grand (Kingpin): But I'm scanning the chip kind of XY and going all the way over the chip to find out what spot is the most susceptible to faults, and then I can focus on that spot to actually defeat the security. So it's like this crazy multi-step process. And I just got this platform

105
00:18:14.450 --> 00:18:22.309
Joe Grand (Kingpin): kind of working last night at 2 in the morning. So I started running it. And that's why I'm on a separate computer. Because I didn't want to mess with the computer that's running that

106
00:18:22.840 --> 00:18:40.320
Joe Grand (Kingpin): that whole platform. So it's pretty cool. And it's like once this, you know, once I complete this work that's going to turn into a talk. And we might do another video more of like. We have this whole backlog now of people that need help with cracking their treasure wallets of all different types. So we thought it'd be fun to do like some sort of Mr. Beast style

107
00:18:40.420 --> 00:18:58.850
Joe Grand (Kingpin): video, that's less technical and more like, look, it's all these different people and tell their stories. But we'll see what happens. But for now it's just like a lot of fun, research. And I I just. I've been sitting here just like watching it every like couple of seconds move. And it's yeah, very satisfying. So hopefully, we'll get some good results out of it.

108
00:18:59.040 --> 00:19:05.589
Ryan Wright: It is so cool to see that you know we met a while ago that you're as excited today

109
00:19:05.970 --> 00:19:11.020
Ryan Wright: staying up till 2 Am. Working on something, and that has never left you. So I just want.

110
00:19:11.020 --> 00:19:17.885
Joe Grand (Kingpin): No, no, and that's and that's that's an important thing, right? It's like, if you find something that that you're into and that you're passionate about

111
00:19:18.620 --> 00:19:26.730
Joe Grand (Kingpin): Then you want to do stuff like that like I really liked it. But I think, on the other hand, too, like I wouldn't be able to do this if Keely wasn't waking up early and bringing the kids to school.

112
00:19:26.730 --> 00:19:27.719
Ryan Wright: So you know.

113
00:19:27.720 --> 00:19:41.120
Joe Grand (Kingpin): We finally figured out this, this thing that works of like I can stay up late and sleep in, and then I'll deal with picking up one of the kids in the afternoon, and then she'll wake up early and do personal training and all the stuff that she does in the morning bring the kids to school. So that

114
00:19:41.430 --> 00:19:45.430
Joe Grand (Kingpin): helps what I do. Otherwise like. If I didn't have kids I would be in here

115
00:19:45.740 --> 00:19:57.510
Joe Grand (Kingpin): 24Â HA day. Right? So it's like, but it is very exciting. And there's sometimes it's not exciting, and it's painful, and it sucks and things break. But that's sort of the nature of hacking in general.

116
00:19:58.550 --> 00:20:11.110
Ryan Wright: So I'll open it up to questions. So just go ahead. Raise your hand. You could even throw it in the chat if you want me to to do? It's talking about. Let's just jump in while while folks are getting queued up here.

117
00:20:11.949 --> 00:20:19.419
Ryan Wright: What what has been like? The hardest project. Then things that maybe you've worked on that that didn't work out for you.

118
00:20:20.690 --> 00:20:21.790
Joe Grand (Kingpin): I mean.

119
00:20:23.340 --> 00:20:30.179
Joe Grand (Kingpin): there's there's been some. My problem is like, if something doesn't work, I'm gonna just keep. It's gonna stay in my head, and I'm gonna keep

120
00:20:30.260 --> 00:20:35.442
Joe Grand (Kingpin): pushing and working on it and working on it and working on it, sometimes sometimes to a negative

121
00:20:36.300 --> 00:20:52.859
Joe Grand (Kingpin): But this this stuff I'm working on now has definitely been hard. I know it's possible. So it's sort of like once that glass ceiling is broken, and you know, something's possible. Then it's just a matter of spending the time to make it possible. So I know that this stuff can be done because I've been

122
00:20:52.860 --> 00:21:08.140
Joe Grand (Kingpin): taking these little steps along the way. But it's definitely this has been one of the longest projects I've worked on of hacking this stuff. There was one once I had a I had a little product that I designed. That was like an RFID reader back when those like Nfc. Reader, that was for, like

123
00:21:08.210 --> 00:21:14.810
Joe Grand (Kingpin): hobbyist electronics which I had done a bunch of hobbyist electronics designed. That was one where we were getting like these really weird failures.

124
00:21:15.240 --> 00:21:17.560
Joe Grand (Kingpin): And it took.

125
00:21:17.860 --> 00:21:42.759
Joe Grand (Kingpin): I think it was 5 years of on and off research to try to figure out what the problem was. And it turns out it was one component on the board. That was the wrong value, and I was throwing off all of like the Rf. Antenna related characteristics, and I went down all these different directions, and I didn't know what it was, but I knew that it would that it shouldn't be doing that right. So I was trying to figure out what this bug was.

126
00:21:42.950 --> 00:22:00.969
Joe Grand (Kingpin): and the fact that it was like a 10 cent part that affected the whole thing like. So there's things like that. There's there's I don't think there's been anything that I've like totally rage quit and been like. I'm never looking at that again unless I I mean, there's been some things that I've really screwed up, and I am trying to fix them. But like.

127
00:22:01.090 --> 00:22:04.520
Joe Grand (Kingpin): yeah, really, the only time I would stop a project is if

128
00:22:04.560 --> 00:22:10.989
Joe Grand (Kingpin): I've exhausted all possibilities. And or there's like some physical damage to the thing that I'm hacking on.

129
00:22:12.680 --> 00:22:27.800
Ryan Wright: Yeah, great. So let's sorry I've been talking too much here. Fascinating things as well. Let's use a couple of hands. Throw it in chat. Let's let's get the conversation going. This is typically a chatty group, but this is our 1st time on zoom together.

130
00:22:27.800 --> 00:22:31.619
Joe Grand (Kingpin): Right. Nobody wants to. Nobody wants to say anything. Yeah, you could type it in the

131
00:22:32.300 --> 00:22:33.840
Joe Grand (Kingpin): type, it in the chat, too.

132
00:22:37.500 --> 00:22:38.250
Joe Grand (Kingpin): Oh, I see.

133
00:22:38.250 --> 00:22:41.219
Ryan Wright: Alright! Thanks for breaking the ice. Kylie! Great job.

134
00:22:42.870 --> 00:23:01.330
Kylee Sanderson: Hi, thank you so much for coming to talk to us today. My question was that you've designed so many really cool things like the universal garage opener the parking meter, smart card emulator. So I was really wondering if you have a favorite project from your whole.

135
00:23:02.070 --> 00:23:03.430
Kylee Sanderson: Career that you.

136
00:23:03.430 --> 00:23:04.360
Joe Grand (Kingpin): Oh, wow!

137
00:23:04.360 --> 00:23:04.950
Kylee Sanderson: Continues.

138
00:23:06.380 --> 00:23:11.405
Joe Grand (Kingpin): They're all favorite in different ways, like they're all special as far as what they are.

139
00:23:11.800 --> 00:23:15.330
Joe Grand (Kingpin): the one the one that comes to mind is

140
00:23:15.350 --> 00:23:44.339
Joe Grand (Kingpin): probably not that exciting, and I have it in my display case. But I don't want to go all the way over there, because there's other stuff in the display case that I probably shouldn't show publicly. I have a device. So for 2 years, when I graduated college, I worked at an engineering company while we were doing the loft. But before we all quit to start the loft, and to start at stake full time, so it was like before you could be a hacker full time. So I was doing engineering work, and I spent 2 years as kind of a junior engineer. But

141
00:23:44.400 --> 00:24:11.229
Joe Grand (Kingpin): but the main person on this project, but sort of overseen by my manager. It was like my 1st real job, actually was my 1st real job doing engineering design. And we had designed a portable infusion pump, like, you know, for people that need to infuse medicine and whatever in a hospital environment. But it was the world's 1st portable one. So normally, you go to hospital, and it's like everything's plugged in. You can't move around.

142
00:24:11.590 --> 00:24:19.309
Joe Grand (Kingpin): This was a device that the patient could actually walk with and move with and bring home and other things. So it was really cool to sort of see

143
00:24:19.470 --> 00:24:27.860
Joe Grand (Kingpin): the medical development process but to get it to the point of of passing all of the FDA testing and

144
00:24:28.490 --> 00:24:44.750
Joe Grand (Kingpin): testing it on actual humans. And it made it made it to an episode of er the TV show. As like they were, they were transporting a patient in a helicopter, and you could see the infusion pump traveling with them, and it was battery powered. And like all these things, that it was just this huge learning

145
00:24:44.800 --> 00:24:46.080
Joe Grand (Kingpin): curve of like

146
00:24:46.710 --> 00:24:55.547
Joe Grand (Kingpin): everything that whole project, from like prototyping to to manufacturing setup and testing and production like

147
00:24:56.100 --> 00:25:18.340
Joe Grand (Kingpin): to be involved in every step of it and sort of been. I was given a lot of leeway and a lot of control of that project where my manager, who is the other engineer, was like kind of handling the client side. So it was this huge learning experience. And I got, you know, was basically learning on the job which affected a lot of my design after that. But it was just such a satisfying project to start it

148
00:25:18.450 --> 00:25:19.890
Joe Grand (Kingpin): from the beginning?

149
00:25:20.450 --> 00:25:45.680
Joe Grand (Kingpin): Do the hardware design do some firmware and like work with other people and do the design reviews? And it. Yeah, it was just a really kind of good comprehensive project that wasn't even hacking related. But it was just like it checked all those boxes of like this is such a cool project, and years later I ended up getting one from the company that I used to work at, and it still works, which is pretty cool. I don't have any medicine to pump through it, but like that was just one that that

150
00:25:45.810 --> 00:25:50.829
Joe Grand (Kingpin): what was so important sort of in so in setting my direction of

151
00:25:51.110 --> 00:25:59.760
Joe Grand (Kingpin): of engineering, which at the same time is, you know, hacking is basically engineering, I think, kind of in reverse but it was just such like a formative

152
00:26:00.210 --> 00:26:05.700
Joe Grand (Kingpin): couple years where I was learning so much like that. That was probably my favorite favorite project.

153
00:26:06.950 --> 00:26:08.030
Ryan Wright: Awesome, Joe.

154
00:26:08.920 --> 00:26:16.759
Ryan Wright: I want to follow up on the education piece because a lot of people think that what you do is completely inaccessible. And yes.

155
00:26:17.700 --> 00:26:33.129
Ryan Wright: but you've told me no, it's actually not inaccessible at all. And you travel the world teaching people how to do hardware hacking. Maybe you can give the the quick elevator pitch on. You know the intro to hardware hacking. And what that's about.

156
00:26:33.330 --> 00:26:43.100
Joe Grand (Kingpin): Yeah. So I mean, so part of my goal in life is to sort of demystify hacking. And that's always how it's been like, even at the loft. You know, we're explaining these things.

157
00:26:43.379 --> 00:26:53.099
Joe Grand (Kingpin): And we always said like, if we had, if we found some security problem like somebody else, probably found it, and is probably exploiting it. So I've just always been trying to let people know like

158
00:26:53.320 --> 00:26:56.059
Joe Grand (Kingpin): this is not out of reach. It just

159
00:26:56.290 --> 00:27:13.549
Joe Grand (Kingpin): it maybe just requires a lot of work, and you know, trial and error. And I think like with everything. Though to me like hacking is much more natural than like playing a guitar or or doing a painting, or you know, any any sort of other creative thing.

160
00:27:14.010 --> 00:27:18.929
Joe Grand (Kingpin): But I think it's something like the videos are intended in that way, too, of like this.

161
00:27:19.440 --> 00:27:26.102
Joe Grand (Kingpin): A lot of times like I said in the video, like feels like magic. It's it's amazing. But it's physics, and it's stuff that

162
00:27:26.540 --> 00:27:41.779
Joe Grand (Kingpin): that anybody can do if they set their mind to it. So you know, on my website, I try to release a lot of information, and with these the hardware hacking classes that I do that is sort of unintentionally become my career. Like all this other stuff I'm doing is just for fun. The traveling when I'm teaching. That's

163
00:27:41.950 --> 00:27:45.789
Joe Grand (Kingpin): that's what I do. And it's crazy to like make a living.

164
00:27:46.350 --> 00:27:48.859
Joe Grand (Kingpin): teaching, talking. Well, you know, you know, it's like.

165
00:27:48.860 --> 00:27:51.399
Ryan Wright: I don't know what you're talking about. I have no idea what you're talking about.

166
00:27:51.400 --> 00:28:03.210
Joe Grand (Kingpin): Love to do like with other people right? And then, you see, you see, all of your students kind of grow and do stuff, and it's like, I remember when they were in my class. It's the same thing of like being able to demystify this.

167
00:28:04.000 --> 00:28:11.969
Joe Grand (Kingpin): Teach people a process or a mindset, or how to use certain tools that they might not have thought about, and then seeing what they go off and do

168
00:28:12.320 --> 00:28:23.260
Joe Grand (Kingpin): it's pretty wild, and a lot of times I'll deal with engineers that are in companies or organizations that I'll never know what they do, because it's secret. But I've heard them say, like, Oh, my God! If if

169
00:28:23.710 --> 00:28:34.356
Joe Grand (Kingpin): I never thought about that like because they're engineering it, they're not thinking about somebody using that stuff against them. So really, it's just that sort of being able to share this stuff and let people know like they could do it.

170
00:28:34.850 --> 00:28:51.830
Joe Grand (Kingpin): we'll hopefully get more people involved in kind of thinking about what technology they're using, how it works and not just relying on the big companies because the big companies don't really have our best interest in mind whether it's Google, whether it's AI, whether it's whatever it is like

171
00:28:52.230 --> 00:28:59.319
Joe Grand (Kingpin): we can, we can use those technologies. But to really understand them. And not rely completely on

172
00:28:59.710 --> 00:29:15.550
Joe Grand (Kingpin): on these big companies to tell us what what stuff should work and how it should work like. I think it's it's fun to sort of, you know. Kick that door down and be like. Actually, you know, I can hack this chip that you say is unhackable like that sort of thing is is fun, and any anybody really can do it if they set their mind to it.

173
00:29:15.790 --> 00:29:17.430
Ryan Wright: Yeah. Thanks. Joe. Anon.

174
00:29:19.960 --> 00:29:36.450
Annan Zulfiqar: Yeah, I just had a quick question regarding hardware crime, like, I guess, of cyber related, is there any like new type of, I guess hardware, like other than like RFID readers, or anything like that that's been coming out using AI or that just kind of worries you like in the hardware space.

175
00:29:37.260 --> 00:29:37.920
Joe Grand (Kingpin): Hmm!

176
00:29:38.160 --> 00:29:39.976
Joe Grand (Kingpin): That's a good question.

177
00:29:40.920 --> 00:29:53.910
Joe Grand (Kingpin): I haven't seen anything from a hardware side that worries me. I've seen some creative stuff like there was, you know, the the car theft where they basically, I can't remember what it was called like. It's their replaying. They're basically like

178
00:29:54.440 --> 00:30:05.187
Joe Grand (Kingpin): they have a big antenna, and they're they're querying like the car key. And then they're replaying that like they're transmitting it to somebody else near the car. Who's who's replaying that to get access to the car.

179
00:30:05.660 --> 00:30:12.205
Joe Grand (Kingpin): I saw one of those in person at at. I was in the Netherlands recently, and some people had brought one in and

180
00:30:12.630 --> 00:30:15.839
Joe Grand (Kingpin): it was interesting because it sort of

181
00:30:16.670 --> 00:30:20.880
Joe Grand (Kingpin): you know, it's it's like any kind of cyber crime group that's taking some

182
00:30:21.160 --> 00:30:37.799
Joe Grand (Kingpin): known exploit and then weaponizing it in some way and with hardware. I think that's usually what happens is cyber crime will or organized crime groups, or whatever it is, will take something if there's a benefit that they can use to

183
00:30:37.840 --> 00:30:44.669
Joe Grand (Kingpin): give them something. So these designs, I thought, were kind of interesting because they were basically kind of mass producing

184
00:30:44.810 --> 00:31:04.049
Joe Grand (Kingpin): a known attack vector which is a replay attacks, but doing that on a scale where it's like something like that, then finally gets, I think, the car manufacturers to listen. So even though it's like, Oh, they're stealing cars, on one hand. On the other hand, it's like, Oh, they're actually they're proving a real world case where

185
00:31:04.140 --> 00:31:06.949
Joe Grand (Kingpin): their hardware design is not secure.

186
00:31:07.461 --> 00:31:21.260
Joe Grand (Kingpin): So maybe the car manufacturers have to step up and like actually design something securely for the key fob so they can't get hacked, which costs money and designing things securely is hard, and that's why a lot of companies sort of don't. But I think it's mostly like

187
00:31:21.490 --> 00:31:28.679
Joe Grand (Kingpin): it. It's more of a curious. I'm more like curious and like it, not impressed. But like

188
00:31:28.840 --> 00:31:51.640
Joe Grand (Kingpin): I just like seeing the hardware that comes out of things like credit card skimmers, the same thing of like seeing how the technology increases over time as law enforcement gets involved. And then the crime groups design something smaller. So it's this cat and mouse game. But I don't think there's anything that scares me. AI, just in general scares me because of how it could be abused, and how humans might

189
00:31:51.680 --> 00:31:53.870
Joe Grand (Kingpin): might rely on it too much.

190
00:31:54.329 --> 00:31:58.279
Joe Grand (Kingpin): From like a design perspective of like somebody's gonna create some code

191
00:31:58.570 --> 00:32:15.490
Joe Grand (Kingpin): they might have AI help in production code, not like Hacker code, like what I'm doing, but production code of like a car or an airplane, or something where they're having AI help too much. And then maybe things are missed in a design review that there's problems later on, or something. But

192
00:32:15.540 --> 00:32:30.889
Joe Grand (Kingpin): hopefully, there's enough. Hopefully, there's enough human oversight with that stuff. But my skeptical side says eventually, we're going to rely on it too much. But yeah, I mean, I think the whole cybercrime space is just fascinating. Just in general.

193
00:32:32.280 --> 00:32:34.229
Ryan Wright: Yeah, thank you. Patrick.

194
00:32:34.800 --> 00:32:51.050
Patrick Apel: Yeah, I'm just curious. You might have like a similar response to this. But like, how do you see hardware hacking, evolving in the future. As I'm sure like. It's changed a lot since like you 1st hacked those telephones and stuff like that. But yeah.

195
00:32:51.540 --> 00:33:08.760
Joe Grand (Kingpin): Yeah, I think. It's it's definitely sort of it's moving forward accessibility. Accessibility to tools is huge. So costs have come down for a lot of things. There's a lot of projects that people can build that give them access to do things so like this. Fault injection stuff I'm doing

196
00:33:08.850 --> 00:33:37.890
Joe Grand (Kingpin): has been known. This technique has been known about for 30 years. But it's only in the past couple of years that it's something that like normal people can do because there is some off the shelf equipment that you can get. That's low cost. There's some open source options that people can build, and it's becoming more understood. So the tools and the resources make it something that more people can get involved in hardware hacking. Some engineers use these same tools to kind of verify their security. But it's mostly like more people will get involved.

197
00:33:37.930 --> 00:33:46.399
Joe Grand (Kingpin): The process of hardware hacking and the process of hacking hasn't changed ever. The technology has changed. So like things have gotten smaller

198
00:33:46.480 --> 00:34:00.276
Joe Grand (Kingpin): components have more functionality integrated into them. So there's not as much that you can kind of probe outside of the board. So your techniques change as far as like how you interact with the hardware. But the process, the mindset's all the same.

199
00:34:00.650 --> 00:34:08.200
Joe Grand (Kingpin): and I think the accessibility to tools is gonna make it easier for people to get involved. But then, at the same time, some devices have been

200
00:34:08.290 --> 00:34:15.989
Joe Grand (Kingpin): have better security features designed in. So yeah, it is the same cat and mouse game, I think, as hardware gets smaller.

201
00:34:16.120 --> 00:34:32.969
Joe Grand (Kingpin): And companies, I would like to say, get more secure. It's gonna be harder, but it's still there's always going to be enough kind of low hanging fruit of devices that we all rely on day to day that aren't as securely designed as mobile phones and game consoles. And, like the, you know, the high end top tier stuff.

202
00:34:34.320 --> 00:34:39.940
Ryan Wright: Yeah, I know we're over time. Do you have time for one more question for me, Joe?

203
00:34:40.360 --> 00:34:54.410
Ryan Wright: I would love to hear about any recommendations you may have for people that are interested in this exploring this like what? What are the best resources out there for for novice.

204
00:34:54.630 --> 00:35:00.939
Joe Grand (Kingpin): Yeah, okay, so I get this question a lot. So much so. And I remember if I mentioned last year. So last year, yeah.

205
00:35:00.940 --> 00:35:03.828
Joe Grand (Kingpin): I'm feeding this question for you.

206
00:35:04.310 --> 00:35:24.418
Joe Grand (Kingpin): So a couple of years ago, actually, I think it's almost 2 years Ben, who's now 16, was like, Daddy, you got to set up a discord server and that way. Like everybody who asked questions, I don't have to respond to individual emails. I could just say, go to my discord. Because with all these videos and stuff like, it's hard to answer everybody anymore.

207
00:35:24.710 --> 00:35:38.800
Joe Grand (Kingpin): so I have this discord server that if you go to. If you just go to joegrand.com that will redirect. I have like a link tree page, and then I think there's a link on there to the discord. But it basically was like an attempt

208
00:35:38.920 --> 00:36:08.910
Joe Grand (Kingpin): to be kind of a communal electronics discussion, hardware hacking discussion forum on there. There's a frequently asked questions list. And there's a bunch of resources of things that are like, how do I get involved in electronics? And it's sort of my recommendation. But really it comes down to just like buying something, experimenting, exploring. And there's a lot of resources like, if you start on my website and grab presentations and then go from there and look at like the references to those and grab other ones. But yeah, on the discord.

209
00:36:09.120 --> 00:36:26.742
Joe Grand (Kingpin): there's a lot of resources. There are a lot of impersonators and scammers, and annoying, like people who are looking for help, recovering cryptocurrency and stuff as well, but like so, it's kind of a mix of like really annoying people, and then, like good people, but very low traffic. But if you want to like, hop in there, grab the frequently asked questions, or whatever

210
00:36:27.480 --> 00:36:35.429
Joe Grand (Kingpin): and and yeah, I mean, like, maybe maybe there's stuff to learn from there and like, or at least you'll get to see kind of, you know

211
00:36:35.530 --> 00:36:37.180
Joe Grand (Kingpin): the underbelly of the

212
00:36:37.250 --> 00:36:51.230
Joe Grand (Kingpin): of the cryptocurrency world, and then, like what? What us of hackers are trying to do once in a while. But for me it's basically like a platform to promote what's coming next, and then answer questions and like guide people to things to kind of free free up my time a little bit.

213
00:36:52.160 --> 00:37:16.640
Ryan Wright: Joe. Thank you so much. One of my favorite things to do, the years to bring you in and into chat with you and our students. And I know you're busy. So I really appreciate you coming in. You'll be very pleased. We have Professor Lewis. Here is gonna talk all things privacy regarding those big companies that you talked about. Well, as we dig into what privacy really means when you're dealing with companies like Google and others.

214
00:37:16.640 --> 00:37:21.240
Joe Grand (Kingpin): Oh, yeah, I mean, that's a whole right. That's a whole other thing just related to trusting companies, right? Is like.

215
00:37:21.860 --> 00:37:26.979
Joe Grand (Kingpin): we don't really know how they're using and abusing our data and everything I did want to show you one thing. Hold on one second.

216
00:37:26.980 --> 00:37:27.640
Ryan Wright: Yeah, yeah.

217
00:37:27.640 --> 00:37:28.860
Joe Grand (Kingpin): We forgot about this.

218
00:37:32.650 --> 00:37:35.080
Joe Grand (Kingpin): Okay, so I know you had mentioned

219
00:37:35.170 --> 00:37:49.190
Joe Grand (Kingpin): Keely's book. So this is this was Keely's book, so Keely is my wife. And that's how we actually met originally. So troublemakers and superpowers. 29 stories of people who turn childhood struggles into strengths. She started writing this book for

220
00:37:49.330 --> 00:37:52.666
Joe Grand (Kingpin): Ben, who was like 4 years old at the time.

221
00:37:53.010 --> 00:38:22.979
Joe Grand (Kingpin): And he was diagnosed eventually with open Ocd. And with anxiety. And so she started writing this book of like, how can kids relate to somebody and like with whatever struggles they have, you know, still turn those into something. So the troublemaker is sort of being, you know, mental health, struggle, or trauma, or whatever. And then the superpowers are like, how can you overcome those in some way? So this is like profile profile book of, like all these different people, you know, ranging from like actual celebrities to pseudo celebrities. So I'm in it.

222
00:38:23.730 --> 00:38:29.689
Joe Grand (Kingpin): That's 1 and but it basically was a way of like, here's the struggles. Here's what they did to overcome them.

223
00:38:30.150 --> 00:38:35.890
Joe Grand (Kingpin): so this is this is really cool. It's not really just for kids like when she wrote my chapter.

224
00:38:35.950 --> 00:38:44.909
Joe Grand (Kingpin): everybody else had an ending to their chapter of like, here's the solution of what they did like I didn't have an ending in here. So I went on, this whole other path of like, you know.

225
00:38:45.030 --> 00:38:50.639
Joe Grand (Kingpin): discovering myself which was cool as a hacker to do that. So that's that book. And then here's

226
00:38:50.790 --> 00:39:03.219
Joe Grand (Kingpin): prototype this. So you'd mentioned the Discovery Channel show. It's kind of hard to see with the glare. So this show was on Discovery Channel, and it was 4 engineers building prototypes of crazy projects. And this was 2,000

227
00:39:03.360 --> 00:39:11.720
Joe Grand (Kingpin): 6, 2,007, I think, or came out? 2,007, 2,008, 2,009. Which is why we were in San Francisco. So this is.

228
00:39:11.830 --> 00:39:25.360
Joe Grand (Kingpin): I think you can get them online. If you go on Youtube, there's like really bad quality versions. But I think you can still go to Discovery Channel and watch them. Or if you are familiar with like using the pirate bay like they're probably on there.

229
00:39:25.470 --> 00:39:46.210
Joe Grand (Kingpin): and or you can go to ebay and like, get a set like I don't make any money on selling them. If I was in class I would just give out, like, you know, thumb drives. But it's a pretty fun show if you're into sort of the engineering building side of things, and it was sort of try. It was filmed by the same production company as mythbusters. So trying to be like sort of a glossy engineering show

230
00:39:46.310 --> 00:39:57.370
Joe Grand (Kingpin): where they were doing science experiments. We were actually designing ridiculous things. In short, amount of time, short amounts of time. And yeah, very, very fun. But that's sort of a side thing. But another way of like sharing

231
00:39:57.720 --> 00:39:59.730
Joe Grand (Kingpin): sharing what you're passionate about.

232
00:39:59.800 --> 00:40:14.890
Joe Grand (Kingpin): I think is the key, because then, that that this show has turned people into like, I've seen people at conferences, and they're like I saw that show. And now I'm an engineer at Apple, or at Google, or whatever, and it's like, Oh, damn like that's pretty cool. So you never know who you're going to inspire in what you do.

233
00:40:15.160 --> 00:40:28.229
Ryan Wright: Yeah, Joe, you're you're a real celebrity. Just so, you know, but I know you don't admit to that. It's good to get a real celebrity in this class. So thank you so much for joining us today. I hope to see you soon. Send my best to the family.

234
00:40:28.230 --> 00:40:32.469
Joe Grand (Kingpin): I will. Yeah. And everybody thanks for coming and and have a good holiday. And yeah.

235
00:40:32.900 --> 00:40:33.929
Joe Grand (Kingpin): talk to you later.

236
00:40:33.930 --> 00:40:34.919
Ryan Wright: Alright! Take care!

237
00:40:34.920 --> 00:40:43.559
Ryan Wright: Alright! See ya bye, all right, Professor Lewis. Was that a good enough introduction for you as well.

238
00:40:44.058 --> 00:40:46.550
Professor Lewis: That was perfect introduction. So

239
00:40:46.720 --> 00:40:50.430
Professor Lewis: alright it's a Monday. It is a Monday, right?

240
00:40:50.905 --> 00:40:54.480
Professor Lewis: You guys can see my screen. I haven't had to ask that all year.

241
00:40:54.880 --> 00:40:57.609
Professor Lewis: Can people see the schedule right now?

242
00:41:02.800 --> 00:41:03.820
Saif Karim: No.

243
00:41:04.100 --> 00:41:06.700
Professor Lewis: No, thank you. Okay. How about now?

244
00:41:07.990 --> 00:41:08.690
Saif Karim: Yeah.

245
00:41:09.110 --> 00:41:33.820
Professor Lewis: All right, that participation. This is great. So 2 classes left. So Joe Graham was just here. We're going to get into privacy. Just a reminder that your last reading quiz is going to be Monday when we come back on December second, and we'll also have Teresa Payton, who very very timely donated her book about cybersecurity in the elections, and you're going to get her. She's pretty famous

246
00:41:33.820 --> 00:41:39.170
Professor Lewis: author, and was the former CIO of the White House. So she is a great person to interact with.

247
00:41:39.710 --> 00:41:53.379
Professor Lewis: Then on Wednesday I will have M. Chu from Dhs. She actually re leads response teams in when companies actually have major cyber incidents for cert under Dhs at the Government.

248
00:41:53.410 --> 00:42:03.249
Professor Lewis: And in addition, we'll do case to our last case. Discussion. I'll get that assignment posted probably today. So that you have some time to look at that as well.

249
00:42:03.780 --> 00:42:13.580
Professor Lewis: Any questions on schedule. We already covered the final exam. Take home you got until the 17.th We're going to open it up directly after class on the 4.th So hopefully, you have

250
00:42:13.720 --> 00:42:19.020
Professor Lewis: a good time period to do that cool.

251
00:42:19.700 --> 00:42:21.519
Professor Lewis: Let's jump in privacy.

252
00:42:22.261 --> 00:42:29.630
Professor Lewis: So, as I promised you I would do. Let's play. Is it real? IoT attacks?

253
00:42:30.178 --> 00:42:36.030
Professor Lewis: Here's 1. So FDA finds Saint Jude. Medical cardiac devices can be hacked.

254
00:42:36.170 --> 00:42:47.190
Professor Lewis: These are cardiac, implantable devices that actually control people's heartbeat. These are pacemakers that can be controlled externally just show of hands who thinks that's real. That seems like a serious one.

255
00:42:48.540 --> 00:43:07.089
Professor Lewis: All right. No electronic hands, one electronic hand. Well, now, you guys are all getting cynical on me. But yeah, that is a real. This is real. So these implanted devices, which they found that the flaw, when they were implanted in numerous numerous thousands of patients.

256
00:43:07.170 --> 00:43:16.429
Professor Lewis: The company that built these things had a problem. Instead of using standards based communications, they had their own, and guess what

257
00:43:16.750 --> 00:43:25.520
Professor Lewis: anyone within 20 feet of that patient. The protocol to control that wireless pacemaker required no authentication of any kind.

258
00:43:25.780 --> 00:43:31.659
Professor Lewis: so that means that anybody that could send a signal could stop or adjust somebody's pacemaker.

259
00:43:31.670 --> 00:43:45.019
Professor Lewis: And so this was kind of a big deal when it came out. But this is the kind of things that we think about IoT attacks. We can kill people pretty easily from within 20 feet. If the security of the devices we're implanting is not sufficient.

260
00:43:46.840 --> 00:43:58.629
Professor Lewis: Wowza. Okay, so for the rest of today, I doubt we'll get through it. We'll go through some privacy. Q&A. The intro to privacy. We'll cover government privacy. We'll cover the law. We'll cover companies and what they're doing.

261
00:43:58.680 --> 00:44:04.910
Professor Lewis: I always feel like somebody's watching me, and after today you will know that someone is always watching you.

262
00:44:05.830 --> 00:44:12.393
Professor Lewis: So we'll start true or false. And I've got polls. I've actually got polls. So

263
00:44:13.730 --> 00:44:18.059
Professor Lewis: Google wants to give you one answer. That's their actual goal.

264
00:44:19.130 --> 00:44:20.070
Professor Lewis: What do you think?

265
00:44:28.380 --> 00:44:36.668
Professor Lewis: Go! We're here. We've hit 30 critical mass. I'll end the poll. We're pretty

266
00:44:37.910 --> 00:44:42.250
Professor Lewis: pretty even there. So 56 to 44 think that's true.

267
00:44:43.386 --> 00:44:45.380
Professor Lewis: That is

268
00:44:46.950 --> 00:44:56.550
Professor Lewis: 100%. So when we're talking about Google, you know, you type something into Google, it gives you 67, 671,000 results for a search term.

269
00:44:56.750 --> 00:44:59.260
Professor Lewis: Well, Google doesn't think that's great.

270
00:44:59.290 --> 00:45:03.949
Professor Lewis: So this is Eric Schmidt, the former CEO of alphabet.

271
00:45:04.310 --> 00:45:06.479
Professor Lewis: And here's what he had to say about it.

272
00:45:16.080 --> 00:45:22.550
Professor Lewis: And and one of my questions leading to is Help us understand, where's the future of surge going? Well.

273
00:45:22.690 --> 00:45:27.430
Professor Lewis: when when you use Google, do you get more than one answer, what should be able to?

274
00:45:27.460 --> 00:45:29.080
Professor Lewis: Well, that's a bug. Yeah.

275
00:45:30.440 --> 00:45:38.779
Professor Lewis: we we have more bugs per second in in the world, because we we should be able to give you the right answer just once

276
00:45:38.960 --> 00:45:40.580
Professor Lewis: we should know what you meant.

277
00:45:40.710 --> 00:45:44.929
Professor Lewis: You should look for information, we should give it exactly right we should give it to you in your language.

278
00:45:45.540 --> 00:45:47.240
Professor Lewis: and we should, and we should never be wrong.

279
00:45:48.140 --> 00:45:59.389
Professor Lewis: We should never be wrong. So Google thinks that they should know you know enough about you that when you type a search term they should give you the one and only one answer that they are looking for.

280
00:46:00.349 --> 00:46:12.569
Professor Lewis: So that's Meta. So our other friends that are, you know, controlling a lot of your corporate privacy online folks like Mark Zuckerberg in 2010, he said, that privacy is no longer a social norm.

281
00:46:13.385 --> 00:46:23.619
Professor Lewis: This same Mark Zuckerberg purchased a home, and the adjacent 4 homes in the Tahoe area to make sure no one had a sight line onto his property.

282
00:46:25.090 --> 00:46:27.710
Professor Lewis: So I'm not making that up.

283
00:46:27.800 --> 00:46:39.840
Professor Lewis: They also scrubbed all the listings so people couldn't see any of the listings for the property. They went to extreme efforts to make sure that no one could invade their privacy on their own personal projects.

284
00:46:40.590 --> 00:46:42.219
Professor Lewis: That same Eric Schmidt

285
00:46:42.290 --> 00:46:54.169
Professor Lewis: said, you're doing something you don't want other people to know. Maybe you shouldn't be doing it. In the 1st place, this is the CEO of Alphabet, a company that knows more about you than almost anyone on earth.

286
00:46:55.990 --> 00:46:57.390
Professor Lewis: All right, these get more fun.

287
00:46:57.490 --> 00:47:05.180
Professor Lewis: so most likely, true or false. Your email has only been read by you and the person that you sent it to.

288
00:47:05.640 --> 00:47:10.020
Professor Lewis: So let's go to the second Poll.

289
00:47:15.870 --> 00:47:22.579
Professor Lewis: my, the classroom is getting much more cynical already, which is good.

290
00:47:23.208 --> 00:47:30.390
Professor Lewis: So we said that. There we are. So 94% think that's false.

291
00:47:30.550 --> 00:47:34.680
Professor Lewis: Well, let's let's give it a shot here.

292
00:47:35.300 --> 00:47:43.230
Professor Lewis: So it depends. Classic faculty answer. But there are false factors meaning that people could read your email. And here they are.

293
00:47:43.360 --> 00:47:49.860
Professor Lewis: So generally your email is sent over the Internet, the public Internet, the unsecure Internet in completely plain text.

294
00:47:49.920 --> 00:47:53.869
Professor Lewis: That is the vast vast majority of all emails that are sent.

295
00:47:54.470 --> 00:48:15.319
Professor Lewis: The government can look at your email if you are a citizen of another country that is indiscriminate. And it doesn't matter if you're a Us. Citizen, if you're emailing a citizen of another country, the government is legally able to intercept and read that. So 4th amendment does not apply in that circumstance.

296
00:48:16.010 --> 00:48:34.220
Professor Lewis: Snowden, Edward Snowden, who we'll talk about again. In addition to programs like this, the Nsa has programs like prism, which are doing huge extra legal, you know, even more illegal things to actually intercept and read things like emails as they traverse the Internet.

297
00:48:35.580 --> 00:48:47.249
Professor Lewis: So in addition to big government, we have to worry about the fact that you agreed to this. So you let Google, you let Yahoo, you let Microsoft read your email when you signed up for a free service.

298
00:48:47.350 --> 00:49:00.259
Professor Lewis: And so if you read those terms and conditions, it means that they can actually monitor your email to advertise to you. That's why your email is free. It's because they're harvesting your information to present you with targeted advertisements.

299
00:49:01.080 --> 00:49:22.380
Professor Lewis: And then data routing quirks. So foreign citizens includes foreign data. And so if just by chance, you're sending an email to someone, and it happens to route outside of the United States that is up for grabs, that means that that can be intercepted by government entities and read legally under the Protect America Act of 2,007.

300
00:49:23.700 --> 00:49:42.679
Professor Lewis: So why might no one read? Why might you be okay? Because many emails never go anywhere. So if you're sending email to another user on office on outlook, if you're sending email onto another Gmail user. It never leaves Google's environment. So it never really crosses the Internet.

301
00:49:43.528 --> 00:49:47.799
Professor Lewis: And in addition, just, you know, to reor reorient to this.

302
00:49:47.820 --> 00:49:58.209
Professor Lewis: the 4th amendment protects you. You have rights. Your right to privacy is extended to your electronic communications. So the government cannot wiretap. You cannot read your emails

303
00:49:58.240 --> 00:50:03.710
Professor Lewis: unless you happen to be corresponding with a citizen of another country.

304
00:50:05.190 --> 00:50:12.200
Professor Lewis: All right, emails, one thing. But now let's talk about text messages which are generally more fun, because

305
00:50:12.410 --> 00:50:20.240
Professor Lewis: sometimes text messages get little bit more interesting. So

306
00:50:20.600 --> 00:50:24.339
Professor Lewis: true or false. Your text messages have only been read by you and your recipient

307
00:50:24.780 --> 00:50:28.189
Professor Lewis: cynical class. Now we're super cynical.

308
00:50:28.290 --> 00:50:30.320
Professor Lewis: So now we're.

309
00:50:30.710 --> 00:50:43.870
Professor Lewis: we're more cynical than we were before. But basically we're looking at 87% think that's not true, which is good because no one ever texts anything racy at all, or suspect or questionable. So that's good for all of us.

310
00:50:44.550 --> 00:50:45.500
Professor Lewis: But

311
00:50:46.053 --> 00:50:54.530
Professor Lewis: let's go into it. So text messages. There are. It's likely that your text messages are private to you.

312
00:50:54.580 --> 00:51:11.389
Professor Lewis: and we'll talk about why. So, this is a kind of a big deal, especially for for students that actually are on folks like their parents plan. So the Consumer telephone Records Protection Act of 2,006 which had a major lawsuit ruled on

313
00:51:12.091 --> 00:51:16.009
Professor Lewis: even if you own the phone and own the bill.

314
00:51:16.130 --> 00:51:23.150
Professor Lewis: you cannot be compelled to make anyone give you their messages because you're invading the privacy of a 3rd party.

315
00:51:23.200 --> 00:51:25.229
Professor Lewis: and you don't know who your child

316
00:51:25.250 --> 00:51:28.450
Professor Lewis: or someone on your phone plan is actually corresponding with.

317
00:51:29.597 --> 00:51:30.759
Professor Lewis: So that's good.

318
00:51:31.279 --> 00:51:46.420
Professor Lewis: In addition, cell phone providers don't store text messages. If they do, it's not for very long. So most text messages are actually, you know, actually, all text messages are client to server. So they're traversing through a provider to get to the person you're sending it to.

319
00:51:47.010 --> 00:51:57.239
Professor Lewis: I'll remind you again about the 4th Amendment. It's illegal, so people cannot look at your text messages without a warrant which hopefully none of us are under warrant surveillance.

320
00:51:57.770 --> 00:52:07.980
Professor Lewis: And then most text messages. Now, if you folks are on imessage. If you're on newer versions of Android, you're actually encrypting those messages as they go across the Internet.

321
00:52:08.000 --> 00:52:13.420
Professor Lewis: which is fantastic, which means that it's really hard for a 3rd party to incept interceptives

322
00:52:14.750 --> 00:52:27.139
Professor Lewis: false factors. If you're texting with a non, Us. Citizen, all bets are off. The Us. Government can intercept that message and look at it anytime they want to, because that's what the Protect America Act says.

323
00:52:27.310 --> 00:52:41.150
Professor Lewis: Once again, data routing text messages. Now don't route cleanly. So sometimes they leave the United States of America. If that happens from a data perspective, that data is available to surveillance and interception by the Nsa and other intelligence bodies

324
00:52:41.500 --> 00:52:45.260
Professor Lewis: and app permissions, errors which we'll talk about in just a second.

325
00:52:46.410 --> 00:52:56.409
Professor Lewis: So you know, just a little background on text messages. There's lots of text messages, dates back to 1984. That's the old text messaging. That's like.

326
00:52:56.804 --> 00:53:07.120
Professor Lewis: you know, when you had your old flip bar your candy bar phones which you guys probably don't remember because you're too young. But you can actually use text messaging on almost any mobile device.

327
00:53:07.830 --> 00:53:10.970
Professor Lewis: Mms is how we introduce things like

328
00:53:11.030 --> 00:53:18.770
Professor Lewis: group chats as well as things like pictures. So texting a picture didn't really come out until 2,002,

329
00:53:19.130 --> 00:53:29.509
Professor Lewis: and then the latest, which is implemented by default on Android now has been on proprietary imessage for a long time is a standard called Rich communication Services.

330
00:53:29.520 --> 00:53:37.200
Professor Lewis: which means that you can see if someone read your message. You know you can see those real time bubbles that something is somebody left you on unread.

331
00:53:37.290 --> 00:53:48.279
Professor Lewis: But you can have media status, real time indicators. It's end to end encrypted. But we need the Internet. So dumb phones cannot use that. So you have to basically have a smartphone.

332
00:53:49.560 --> 00:54:01.250
Professor Lewis: So in addition to text messages, imessage, SMS, Rcs, there are texting type applications, things like Whatsapp, Facebook Messenger, wechat, etc, etc.

333
00:54:01.260 --> 00:54:06.760
Professor Lewis: All of these were created in one way or another about your privacy.

334
00:54:07.646 --> 00:54:21.159
Professor Lewis: So as we see this chart here, we have different components, and what the companies that control them have agreed to do. And there's a big checkmark up here from telegram which is gone now.

335
00:54:21.260 --> 00:54:37.069
Professor Lewis: So telegram for years and years said, You can go to hell authorities. We are not going to do anything to help you surveil our users. All you had to do was take the founder and throw him in jail in France, and all of a sudden they got very willing to help authorities look at these messages.

336
00:54:37.850 --> 00:54:58.749
Professor Lewis: So if you're really concerned about your privacy, I recommend that you move down to the right side. The signal app actually uses the signal protocol which actually uses encryption, separate encryption on every single packet that's sent, which is amazingly hard to break, because you basically have to break encryption every single time.

337
00:54:59.840 --> 00:55:02.900
Professor Lewis: And if you're really, really paranoid

338
00:55:03.768 --> 00:55:15.031
Professor Lewis: you might know that certain entities like encroak chat, which is re, was really popular in Europe, especially on criminal elements. Well, that was actually run by the authorities.

339
00:55:15.460 --> 00:55:20.560
Professor Lewis: So engrochat is a good example of a text messaging application that you

340
00:55:20.580 --> 00:55:23.920
Professor Lewis: unwillingly might have used to send.

341
00:55:24.322 --> 00:55:39.629
Professor Lewis: You know, legitimate, illegitimate traffic to your friends and colleagues. Well, the Government was actually looking at all of that traffic, and this was a huge sting operation. They ended up breaking down a lot of criminal gangs in Europe.

342
00:55:39.670 --> 00:55:54.929
Professor Lewis: when they revealed that encro chat was actually created by the Government for the purpose of surveilling people so marketed as a very secure platform. You should use this. And then under the covers, it was actually run by law enforcement cool. Huh?

343
00:55:55.700 --> 00:56:12.989
Professor Lewis: If you're super super paranoid like, I don't know Professor Wright, you might use an application like briar. Most of these applications are client server. So, like your your message is actually going through a 3rd party who's providing the service to the recipient

344
00:56:13.130 --> 00:56:16.269
Professor Lewis: breyer is not. There is nobody in the middle.

345
00:56:16.410 --> 00:56:30.460
Professor Lewis: And so Briar has a really unique architecture in that it works. It can work online or offline. And so when you post a message, it can actually move laterally via Bluetooth, it can move via the Tor network.

346
00:56:30.650 --> 00:56:37.859
Professor Lewis: And then the origins of who actually wrote that message are never actually known to 3rd parties.

347
00:56:37.950 --> 00:56:54.650
Professor Lewis: So brier is an attempt, you know. It seems like something terrorist networks would use all the time, but it's also used by people that really value their privacy. Folks like journalists, folks that are trying to investigate things that you know, the government might not want exposed.

348
00:56:55.124 --> 00:56:59.239
Professor Lewis: So Briar is a really, really interesting. I've been following these guys for a while.

349
00:56:59.650 --> 00:57:04.450
Professor Lewis: Really, really interesting way that they've approached it so that there's no

350
00:57:04.783 --> 00:57:17.219
Professor Lewis: there's basically no server in the middle. So there's no one the government can go compel and say, Hey, you know I need access to this for these reasons. But there's no one to subpoena. Basically, it's all peer to peer

351
00:57:20.200 --> 00:57:24.391
Professor Lewis: alright. So in addition to you know actual surveillance.

352
00:57:25.050 --> 00:57:39.289
Professor Lewis: Whenever you install a new app, it asks you for permissions, and most of you look at that very, very carefully right, and you read every line. And you think about it. And you say, Yeah, that doesn't make sense. I'm not going to do that.

353
00:57:40.160 --> 00:57:44.049
Professor Lewis: Or maybe you just say, Okay, okay, okay, I want to install that.

354
00:57:44.380 --> 00:57:52.049
Professor Lewis: And if you said that that app can read your text messages, it means it can read all of your text messages.

355
00:57:53.090 --> 00:58:10.190
Professor Lewis: In addition, a lot of apps actually don't disclose what they actually have permission to. And so there are example after example of legitimate apps that actually have data overflows where they may have access to all of your text messages on your phones.

356
00:58:11.630 --> 00:58:12.500
Professor Lewis: Sorry?

357
00:58:15.560 --> 00:58:18.650
Professor Lewis: Alright. So you know, in addition to all of that.

358
00:58:19.090 --> 00:58:28.510
Professor Lewis: your telemetry data, which is the data used to track your location. You, you're really paranoid. So you say, Hey, Google, don't track my location.

359
00:58:28.750 --> 00:58:39.790
Professor Lewis: Well, guess what the cell phone companies still know where your phone is, because they use triangulation to actually, you know, keep keep tracks of where that phone is as you move between towers.

360
00:58:39.930 --> 00:58:53.479
Professor Lewis: So that information the major cell phone providers will sell to 3rd party data brokers. And then folks like bounty hunters if they're actually trying to find somebody. They can go buy that from a data broker and then go find that phone.

361
00:58:54.000 --> 00:58:59.609
Professor Lewis: And so ultimately, if someone knows your phone number, they can probably find where you are on Earth.

362
00:59:04.660 --> 00:59:10.790
Professor Lewis: All right. So polls again. We'll go to the government here. So the

363
00:59:12.450 --> 00:59:24.479
Professor Lewis: I'm going to say the annual Us. Budget for covert data collection is bigger than Gdp. Of all these companies, we're all cynical at this point. Professor Lewis has ruined us on everything.

364
00:59:24.870 --> 00:59:29.419
Professor Lewis: Yeah, okay, so we can end that poll. We can share the results.

365
00:59:29.460 --> 00:59:37.640
Professor Lewis: We can say, everybody thinks that's true. And that is super true. You know why we know that Snowden. So all of these black Ops

366
00:59:38.445 --> 00:59:41.920
Professor Lewis: budgets are not published to.

367
00:59:41.950 --> 00:59:49.709
Professor Lewis: but anyone, basically not to the United Citizens of the United States. It was revealed by Snowden, a massive, massive

368
00:59:49.770 --> 01:00:00.410
Professor Lewis: amounts of surveillance funding for things like the Nsa. The Nro Dsi. Other organizations whose job is to collect intelligence.

369
01:00:03.290 --> 01:00:10.460
Professor Lewis: All right, we'll go down the same path. 9 out of 10 people whose data are collected

370
01:00:11.120 --> 01:00:14.839
Professor Lewis: is ordinary Americans, not the intended targets.

371
01:00:20.380 --> 01:00:28.410
Professor Lewis: cynical, cynical, cynical, cynical, cynical. 9 out of 10 of you think that's true.

372
01:00:28.620 --> 01:00:31.749
Professor Lewis: Snowden tells us that is true.

373
01:00:32.370 --> 01:00:37.170
Professor Lewis: Senate hearings on the Snowden leaks. Tell us that's true.

374
01:00:37.720 --> 01:00:40.279
Professor Lewis: so we'll go to General Michael Hayden.

375
01:00:43.800 --> 01:00:45.049
Professor Lewis: Maybe we won't.

376
01:00:47.460 --> 01:00:50.100
Professor Lewis: You know what we won't, as it looks like.

377
01:00:56.350 --> 01:00:58.229
Professor Lewis: No, we're not going to do that.

378
01:00:58.865 --> 01:01:06.300
Professor Lewis: So Michael Hayden here. Who you met before earlier in the class he is famously quoted as

379
01:01:06.390 --> 01:01:17.999
Professor Lewis: we kill people based on metadata. And so when they're collecting data on a target, they're actually collecting data on that target. And anyone that that target interacts with. And so

380
01:01:18.020 --> 01:01:22.420
Professor Lewis: if a friend of a friend is interacting, then your data is up for gain.

381
01:01:22.450 --> 01:01:25.160
Professor Lewis: So all of that is true.

382
01:01:26.710 --> 01:01:32.299
Professor Lewis: Alright last one, unless this actually works. Yay, hey! Here we go.

383
01:01:33.460 --> 01:01:34.939
Professor Lewis: And so here's Michael Hayden.

384
01:01:40.060 --> 01:01:47.440
Professor Lewis: This is all done under the Foreign Intelligence Surveillance Act and the Foreign Intelligence Surveillance Court is the secret court that

385
01:01:47.570 --> 01:01:54.810
Professor Lewis: ways, as I said in secret the legal justification for

386
01:01:55.540 --> 01:02:00.509
Professor Lewis: putting together a search of metadata linked to a person or persons.

387
01:02:00.570 --> 01:02:06.600
Professor Lewis: and then gives it an okay or not. Okay. And it is on a time, general hate.

388
01:02:06.930 --> 01:02:11.910
Professor Lewis: You can find out all these things David just said about me, about David, about anyone in this audience.

389
01:02:13.000 --> 01:02:33.850
Professor Lewis: That is a function of operational capability. I'd like you to talk about whether you're comfortable with that operational capability. If so, why and how often is it used in the ways that David described. Yeah, 1st of all, David's description of what you can do with metadata and boarding a mutual friend, Stuart Baker, is absolutely correct.

390
01:02:33.870 --> 01:02:36.930
Professor Lewis: and we kill people based on metadata.

391
01:02:40.300 --> 01:02:44.929
Professor Lewis: That's the Nsa. Director, Michael Hayden. We kill people based on metadata.

392
01:02:45.990 --> 01:03:01.249
Professor Lewis: All right last one fun one. If you've never been to the Nsa. Headquarters. You probably will never go to the Nsa. Headquarters. In the past we have taken student groups to the CIA who do student tours the Nsa. Does not.

393
01:03:01.840 --> 01:03:10.289
Professor Lewis: So here's the quote. It's right over the front door right there as you walk in. I've been there a couple of times. Am I right?

394
01:03:10.680 --> 01:03:12.700
Professor Lewis: Or did I misquote it?

395
01:03:17.200 --> 01:03:20.099
Professor Lewis: Oh, I'm still sharing a result. Sorry.

396
01:03:33.910 --> 01:03:35.180
Professor Lewis: Wow, okay,

397
01:03:37.250 --> 01:03:45.980
Professor Lewis: there you go. 96% of you were wrong. Come on. That's a quote from 1984 that that's a little bit too dystopian, isn't it? Come on.

398
01:03:46.200 --> 01:03:55.139
Professor Lewis: So that's 1984. And that that quote is not on the on the Nsa building. I'm just being a little bit

399
01:03:55.840 --> 01:03:57.570
Professor Lewis: little bit over the top here.

400
01:03:58.520 --> 01:04:00.780
Professor Lewis: George Orwell, 1984.

401
01:04:03.270 --> 01:04:15.089
Professor Lewis: Okay, so privacy quiz over. We have 10Â min, so we're certainly not going to get through privacy, but we'll start us, and then set us on our course to think about some things over Thanksgiving break

402
01:04:15.360 --> 01:04:28.209
Professor Lewis: in the Us. We have no single comprehensive Federal national law regulating the collection use of personal data. We have lots of laws at the state level, we're up to 8 that have comprehensive privacy laws.

403
01:04:28.240 --> 01:04:30.699
Professor Lewis: Obviously California was the 1st one.

404
01:04:31.560 --> 01:04:47.080
Professor Lewis: But a lot of these laws actually contradict each other. These laws make it very difficult for companies and corporations that do business in across the country to do business, because many of the laws like Oregon's laws.

405
01:04:47.360 --> 01:05:01.499
Professor Lewis: They deal with Oregon citizens wherever they are. So if an Oregon citizen is traveling to New York, then a New York company, collecting their data must adhere to Oregon State law, which, as you can imagine, is a lot of fun for companies to deal with.

406
01:05:02.550 --> 01:05:16.509
Professor Lewis: So what laws are we talking about? Well, a lot of them, especially about your privacy, are controlled by the ftc, so the Ftc. Has broad enforcement about misuse of your personal data at the Federal level

407
01:05:17.585 --> 01:05:26.319
Professor Lewis: at the financial level. Fisma gram leach bliley act affects finance for financial services, firms which

408
01:05:26.410 --> 01:05:37.370
Professor Lewis: the privacy and how they actually hold records the requirement that they actually hold things like cisos and chief privacy officers are contained in that act.

409
01:05:37.910 --> 01:05:49.719
Professor Lewis: hipaa, which we'll talk about a lot next time which we've talked about before is about protected health information as well as a lot of other things, but for the purposes of this course is just about privacy

410
01:05:50.487 --> 01:06:01.279
Professor Lewis: can spam is an important act in terms of how companies can push market at you doesn't have much to do with your data, but it's about how they can spam you.

411
01:06:01.770 --> 01:06:20.670
Professor Lewis: And then the granddaddy of them all is the Ecpa, which, if you remember, we're prosecuting a bunch of Chinese citizens for that equifax breach. We're actually doing it under an act from 1986. Because what this act does is it extends your expectations of privacy and records

412
01:06:20.670 --> 01:06:34.799
Professor Lewis: from your household, from your body to your electronic communications. So the Ecpa is really the granddaddy of them all in terms of how we can prosecute abuses of personal privacy.

413
01:06:36.300 --> 01:06:45.660
Professor Lewis: We do things a lot different in the United States. So in basically, most other countries, you have to opt in to receive emails

414
01:06:45.680 --> 01:06:51.699
Professor Lewis: from a corporation United States. You don't have to opt in you just have to have the ability to opt out

415
01:06:52.954 --> 01:07:05.230
Professor Lewis: if we violate things, the Can Spam act in the United States, they can find spammers up to $16,000 per email for spamming, unsolicited messaging.

416
01:07:05.740 --> 01:07:22.929
Professor Lewis: It doesn't really happen. Can spam is almost unprosecutal because it's hard to find out where emails originated. So in other countries, Canada, up to 10 million dollars per fine there for unsolicited marking over email.

417
01:07:24.200 --> 01:07:26.560
Professor Lewis: So what about text messaging?

418
01:07:26.650 --> 01:07:29.870
Professor Lewis: Can companies text you just text you randomly.

419
01:07:30.598 --> 01:07:32.212
Professor Lewis: No, they cannot.

420
01:07:32.840 --> 01:07:44.399
Professor Lewis: They must obtain consent to send messages to consumers. They have to have explicit written permissions. Once you are opted into a text messaging service.

421
01:07:44.410 --> 01:07:51.180
Professor Lewis: you they have to disclose you what they're going to text you about and provide a way for you to opt out. If they do not

422
01:07:51.637 --> 01:08:02.240
Professor Lewis: they can be fined up to $500 per text under Ftc rules. So those are pretty pretty healthy fines from companies sending unsolicited messages.

423
01:08:03.620 --> 01:08:11.410
Professor Lewis: So in terms of your personal privacy, is this getting better? So at the

424
01:08:11.610 --> 01:08:19.880
Professor Lewis: yeah. President Trump actually repealed a set of Fcc regulations about

425
01:08:19.950 --> 01:08:25.839
Professor Lewis: broadband Internet privacy. What your actual isps can do when they're looking at your data

426
01:08:26.620 --> 01:08:36.539
Professor Lewis: that isn't still, not on the books. So we we do not

427
01:08:36.560 --> 01:08:52.720
Professor Lewis: currently, in United States law, actually treat things like browsing history or app usage history as sensitive data. So companies can basically feast upon that to better serve you in terms of the products they're offering you and the advertisements they're sending you.

428
01:08:53.899 --> 01:09:03.370
Professor Lewis: It's a little bit different across the pond. Either pond. So consumer privacy is more definitely more highly valued outside of us. Law.

429
01:09:03.910 --> 01:09:21.410
Professor Lewis: you may have heard of. Gdpr. You may have noticed all those damn banners that I have to opt into cookies. All of that is due to Gdpr. And the follow up State laws like the Ccpa. But this is the idea that I have to give permission to actually

430
01:09:21.470 --> 01:09:23.330
Professor Lewis: for people to track me.

431
01:09:23.859 --> 01:09:31.399
Professor Lewis: So what's the right to be forgotten. Anybody want to venture a guess? Anybody's heard about Gdpr and the right to be forgotten.

432
01:09:34.029 --> 01:09:38.369
Professor Lewis: I haven't taught on Zoom in a while this is weird can see everybody.

433
01:09:38.829 --> 01:09:44.420
Professor Lewis: Anybody want to wade into that? Anybody know what? The right to be forgotten. Is anybody heard the term before?

434
01:09:46.649 --> 01:09:48.970
Professor Lewis: Wow, okay.

435
01:09:50.569 --> 01:10:08.020
Professor Lewis: Might be the Monday before Thanksgiving. But the right to be forgotten means that under Gdpr. Rules as well as Ccpa. Rules California residents, it means that if I don't want a company to maintain any records of my interactions with that company, I can request that they expunge. The data

436
01:10:08.880 --> 01:10:26.649
Professor Lewis: creates very interesting legal questions. So we've had a number of conversations at places like, I don't know the University of Virginia about what if I apply to the University of Virginia, and then I invoke my right to be forgotten, and then I don't have

437
01:10:26.760 --> 01:10:30.549
Professor Lewis: any record that I applied, and then I could apply again.

438
01:10:30.890 --> 01:10:51.990
Professor Lewis: So those sorts of things are actually ruled out in the courts, the lawyers figure those out. And so at the University of Virginia we have decided that that is a critical business need, and if, even if you ask us to not have any record that I applied, we will keep a record that you applied, and we are legally in the right to do so, even if you're a European citizen.

439
01:10:54.260 --> 01:11:08.179
Professor Lewis: So keeping down the path. So this is a some friends of mine own. A company called Display Note. They're based up in Northern Ireland. So they are subject to. Gdpr.

440
01:11:08.280 --> 01:11:09.870
Professor Lewis: Well, they were. But

441
01:11:10.207 --> 01:11:30.809
Professor Lewis: this is their you know, you're accepting their cookies. There's this whole science about how you get people to opt into things that normally they wouldn't. 1 of them is. So I have to click this more information button to not opt in. Or I could just click this giant green button and then get to the website. I'm trying to get to.

442
01:11:31.110 --> 01:11:35.750
Professor Lewis: Well, as you can imagine, about 99% of people just click, accept

443
01:11:35.960 --> 01:11:42.239
Professor Lewis: unless you're me, and then you get to read about who's tracking you. So on display note.

444
01:11:42.680 --> 01:11:50.809
Professor Lewis: And there are 21 different data tracking entities on a single website that are tracking what you're doing on that website.

445
01:11:51.200 --> 01:12:01.290
Professor Lewis: And this is low. A lot of websites are in the triple digits of different data harvesting engines that are looking about how you interact with that website.

446
01:12:02.200 --> 01:12:03.490
Professor Lewis: Pretty remarkable, huh?

447
01:12:03.540 --> 01:12:05.089
Professor Lewis: Well, how do they do that?

448
01:12:05.800 --> 01:12:08.210
Professor Lewis: So one of the ways they do that

449
01:12:08.260 --> 01:12:11.189
Professor Lewis: is a concept known as cookies.

450
01:12:11.360 --> 01:12:17.699
Professor Lewis: and we'll finish cookies, and then we'll pick up this conversation after Teresa on Monday.

451
01:12:17.750 --> 01:12:37.880
Professor Lewis: But there are different types of cookies and cookies are pretty simple. They're just text files that can be stored on your machine. And there are different types. So there are cookies that basically get written while you're on that website. There are cookies that basically are written forever and kept on your website unless you delete all your cookies.

452
01:12:37.880 --> 01:12:54.829
Professor Lewis: And then there are 3rd party cookies, so that display. Note the use of those 21 different data collecting websites? Those are 3rd parties that are looking at your data. It's not who you're trying to do business. It's just other people that may be interested in how you're looking at that information.

453
01:12:56.030 --> 01:13:14.160
Professor Lewis: We'll talk about tech. So the big browser companies have said, they're gonna you know, basically stop 3rd party cookies for a long time. It's supposed to happen this year. Don't hold your breath. I don't think 3rd party cookies are actually going to be discontinued in the near future.

454
01:13:14.830 --> 01:13:29.649
Professor Lewis: But in terms of text, we'll talk about pixel tags. Ultrasound beacons are really interesting because they can actually follow you around physical environments like malls if malls existed but stores and other places

455
01:13:30.084 --> 01:13:36.474
Professor Lewis: and then browser fingerprinting is this idea that when we get to it. I didn't log in

456
01:13:36.840 --> 01:13:47.809
Professor Lewis: I'm operating in incognito mode. No one can track who I am. Well, we'll find out that that's not true at all, and almost every one of you in this room can be tracked.

457
01:13:48.182 --> 01:13:53.670
Professor Lewis: Just by visiting any website, no matter what you're doing to try to protect your privacy

458
01:13:56.190 --> 01:14:10.980
Professor Lewis: all right, 1Â min. So we'll talk about cookies. So I visit a website. What that website is able to do is send me a cookie, which is basically just a text file. We can look at them if you really want to, but then it it stores on your hard drive.

459
01:14:11.790 --> 01:14:33.359
Professor Lewis: so that that cookie is there this piece of text? So the next time you visit that website it knows that it was actually Alex. And so Amazon says, Hey, Alex. Welcome back, you know. Would you like to go back to your saved cart? That's really handy? Right? So it makes sure that your information is there, and that

460
01:14:33.420 --> 01:14:43.280
Professor Lewis: you are identified, that it's you visiting something which is really kind of straightforward for a 1-to-one. I'm trying to interact with a 3rd party website.

461
01:14:43.470 --> 01:14:58.760
Professor Lewis: The problem is that other websites can also read those cookies, and as a result, you know, they can start seeing your browsing history. They can start putting together what you did when they can gather a lot of information on who you are and what you're interested in.

462
01:15:00.290 --> 01:15:15.169
Professor Lewis: So we look at it a little bit further. So in some cases it's not just text. And so cookies, you know, are just text files, but they can contain pieces of code, you know, simple code like Javascript.

463
01:15:15.270 --> 01:15:39.239
Professor Lewis: And as a result, you know, we can start using technologies such as tracking pixels, which tracking pixels are a great great way to know if I visited anything. Tracking pixels are often used in emails to know. Want to know if you open the email, you know, how do they know that I opened this email. Well, when I open that email, that one little pixel that I couldn't even see it called to a website

464
01:15:39.240 --> 01:15:54.000
Professor Lewis: and said, Hey, I need this image file. And that little time one little pixel makes it possible for them to actually track that you opened a website or visited a different website. They're often used in things like advertisements on a website

465
01:15:54.120 --> 01:15:57.850
Professor Lewis: to know that it was you that actually opened that page.

466
01:16:00.570 --> 01:16:07.429
Professor Lewis: So we're a minute over. So I'm gonna hold there.

467
01:16:07.710 --> 01:16:11.260
Professor Lewis: And I'm gonna say, Happy Thanksgiving.

468
01:16:11.620 --> 01:16:18.309
Professor Lewis: I appreciate everybody's attention today and great questions. For our speaker. So

469
01:16:18.450 --> 01:16:29.180
Professor Lewis: unless there's any questions, I will say, thank you all happy Thanksgiving. Enjoy the break hopefully. You get to spend some time offline, and we'll see you in a week.

470
01:16:32.650 --> 01:16:34.370
Ashley Pham: Thank you. Thank you.

471
01:16:34.370 --> 01:16:34.870
Kyle Woodson: Zoom.

472
01:16:34.870 --> 01:16:35.960
Tristan Chang: Thank you.

473
01:16:35.960 --> 01:16:36.730
Saif Karim: Thank you.

474
01:16:40.960 --> 01:16:42.479
Professor Lewis: And Neil, you staying with me.

475
01:16:45.110 --> 01:17:01.180
Neel Kulkarni: Well, yeah, I think I emailed you guys about it. But just missing. Was it? December second lecture? I know we have the reading quiz and I know, like for participation, I can follow that form. So just wanted to make sure

476
01:17:01.810 --> 01:17:02.510
Neel Kulkarni: that was all.

477
01:17:02.510 --> 01:17:07.249
Professor Lewis: It's yeah. Fill out the form. Did you already fill out the absence form?

478
01:17:07.460 --> 01:17:09.270
Neel Kulkarni: No, I can. I can do that right off.

479
01:17:09.587 --> 01:17:25.122
Professor Lewis: When you thought it triggers the reply, there's there'll be a recording, and then I'll just open up the reading quiz for you. So you can do it anytime that day. Cause. I know you're in an interview. So I want you focused on that. Okay.

480
01:17:25.440 --> 01:17:27.669
Neel Kulkarni: Yeah. Alright. Thank you.

481
01:17:27.670 --> 01:17:28.599
Professor Lewis: All right. Neil. Sounds good.

482
01:17:28.600 --> 01:17:29.529
Neel Kulkarni: Yeah. Take care.

483
01:17:34.750 --> 01:17:35.640
Professor Lewis: Sonny.

484
01:17:35.940 --> 01:17:36.853
Sunny Singh: What's up?

485
01:17:37.390 --> 01:17:41.840
Sunny Singh: yeah. We me and Stan want to ask you guys a couple of questions about like what we tried.

486
01:17:41.850 --> 01:17:43.970
Sunny Singh: And like, I just get like a better idea.

487
01:17:44.270 --> 01:17:57.799
Sunny Singh: for you know what you look for. Cause. So was actually in that club previously, and he said that that's exactly how they reached out to professors with like the same email and everything. And so we were kind of curious about like from your perspective, what stood out.

488
01:17:58.230 --> 01:18:06.638
Professor Lewis: Well, Professor Wright is meaner than I am. I just look at it and laugh, but some of us are a little bit more mean spirited. But

489
01:18:07.675 --> 01:18:12.334
Professor Lewis: let me say it's not the 1st time that's been

490
01:18:12.900 --> 01:18:22.512
Professor Lewis: requested. So there's a couple of things about that message that are. You know it. It's fairly textbook in terms of

491
01:18:23.340 --> 01:18:34.810
Professor Lewis: you know, we're not a person. We're an organization like there are some some things. And then, looking at the links is is kind of a way to do it. But I did get a laugh. I did get a laugh.

492
01:18:34.810 --> 01:18:35.610
Sunny Singh: That's good.

493
01:18:36.470 --> 01:18:41.290
Ryan Wright: No, I would. I think if you send that to 10 professors, you're gonna get 9 responses. Frankly.

494
01:18:41.290 --> 01:18:48.719
Professor Lewis: You would. And you know you're preying on something great, Professor egos, because, you know, hey, someone wants to talk to me because I'm an expert.

495
01:18:49.077 --> 01:19:06.129
Professor Lewis: That that's a great motivational factor, you know, if we look at Cialdini's like factors for for how we get people to act on information. So I I don't know. I didn't know who it came from, but I had a good ideas from somebody in the class, so.

496
01:19:06.130 --> 01:19:11.120
Sunny Singh: Well, yeah, I mean, I I guess I assume you didn't click on the Linkedin. But we like put our names on there. It's just like a picture.

497
01:19:11.120 --> 01:19:15.462
Professor Lewis: No, no, just I don't click on anything.

498
01:19:15.950 --> 01:19:16.730
Sunny Singh: Smart, smart.

499
01:19:16.730 --> 01:19:23.059
Professor Lewis: But I do. I have this big fishing folder, and I actually dragged it into my fish folder. And I was like Hmm!

500
01:19:23.150 --> 01:19:39.849
Professor Lewis: And you know, with a new Ciso here I honestly didn't know if it was him, because in some of my conversations with him he said, he's gonna undertake more targeted, because in the past our fishing simulations are one, you know, one message to 10,000 people.

501
01:19:40.463 --> 01:19:44.679
Professor Lewis: But I I it isn't sitting in my fish folder so.

502
01:19:45.190 --> 01:19:55.120
Professor Lewis: But I applaud you. The actually, some cyber security classes. They actually throw the gauntlet, and they're like, if you fish me, then you get an a this semester.

503
01:19:55.220 --> 01:19:56.786
Professor Lewis: We we don't do that.

504
01:19:57.100 --> 01:19:59.300
Ryan Wright: No, because we everybody would get an.

505
01:19:59.300 --> 01:19:59.900
Professor Lewis: Everybody. Yeah.

506
01:20:01.164 --> 01:20:11.535
Professor Lewis: but it's funny I was with Mike Higgins, this weekend, and he's a former Ciso at 20 companies. He's 1 of the most incredible guys that you'll ever talk to.

507
01:20:12.010 --> 01:20:15.140
Professor Lewis: But he described when

508
01:20:16.058 --> 01:20:20.690
Professor Lewis: you know the new, he had a new director come in at dhs and say.

509
01:20:20.740 --> 01:20:27.960
Professor Lewis: click! Clicking on a phishing email is a fireable offense. And so the 1st thing he did at Ciso was phish him

510
01:20:28.260 --> 01:20:35.520
Professor Lewis: and guess what the story ends there. But all of a sudden that fireable offense thing was lifted. Because

511
01:20:36.010 --> 01:20:43.669
Professor Lewis: it's gonna happen, you know, and it's not that you're an idiot. It's that, you know, they're they're very compelling. So.

512
01:20:43.890 --> 01:20:46.849
Professor Lewis: But I appreciate the creativity guys. It was.

513
01:20:47.520 --> 01:20:52.220
Snehan Biswas: So as a club ever like reached out to you guys before to like, speak. And how have they done it?

514
01:20:53.050 --> 01:20:54.060
Professor Lewis: So.

515
01:20:55.245 --> 01:21:03.443
Professor Lewis: Yes, for sure, I mean generally. There's a little bit of back and forth with

516
01:21:03.960 --> 01:21:09.669
Professor Lewis: With that I actually Ryan I who was the last

517
01:21:10.220 --> 01:21:14.070
Professor Lewis: I mean. It had to be like when the you just spoke it.

518
01:21:14.160 --> 01:21:17.619
Professor Lewis: That was that was a little while ago. But I'll let you go.

519
01:21:18.920 --> 01:21:19.866
Professor Lewis: It was

520
01:21:20.740 --> 01:21:22.090
Professor Lewis: Where did you just speak?

521
01:21:22.200 --> 01:21:23.540
Professor Lewis: It was at.

522
01:21:23.860 --> 01:21:28.240
Ryan Wright: Oh, you mean, like, for the Us Department of transportation, that talk. Yeah.

523
01:21:28.930 --> 01:21:35.169
Ryan Wright: that's a little bit different. Yeah. When the undersecretary emails, you, you typically click on those links.

524
01:21:35.170 --> 01:21:35.850
Sunny Singh: Yeah.

525
01:21:36.320 --> 01:21:38.970
Snehan Biswas: But like a club at Uva. Anything like that.

526
01:21:38.970 --> 01:21:42.249
Ryan Wright: Yeah. So I've done a couple of those.

527
01:21:42.900 --> 01:21:49.860
Ryan Wright: yeah, I think. Yeah, we we've done a couple of those. Those are all dependent on scheduling right? That's usually the hardest thing

528
01:21:50.486 --> 01:21:56.739
Ryan Wright: to do that. But yeah, I think Brian and I have done several of those throughout the years.

529
01:21:58.900 --> 01:22:01.659
Sunny Singh: Alright. Well, we just wanted to ask. But thank you, guys.

530
01:22:01.860 --> 01:22:04.160
Ryan Wright: Yeah, have a good Thanksgiving, and we'll.

531
01:22:04.160 --> 01:22:05.959
Professor Lewis: West. I mean, we're we're happy to.

532
01:22:05.960 --> 01:22:08.129
Sunny Singh: Oh, no, no! It was just part of like a we.

533
01:22:08.130 --> 01:22:08.790
Professor Lewis: Oh, no, we don't.

534
01:22:08.790 --> 01:22:09.199
Sunny Singh: And we don'.

535
01:22:09.200 --> 01:22:09.600
Professor Lewis: Guys.

536
01:22:09.955 --> 01:22:12.090
Sunny Singh: Well, maybe maybe in the future.

